{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Erkennen und Analysieren von Gesichtern\r\n",
        "\r\n",
        "Lösungen für maschinelles Sehen benötigen oft eine Lösung für künstliche Intelligenz (KI), um menschliche Gesichter erkennen, analysieren oder identifizieren zu können. Angenommen, das Einzelhandelsunternehmen Northwind Traders möchte einen „Smart Store“ implementieren, der von KI überwacht wird, um zu identifizieren, welche Kunden Unterstützung benötigen, und Mitarbeiter zu diesen Kunden zu schicken. Dieses Ziel kann mit der Erkennung und Analyse von Gesichtern erreicht werden. Dazu müssen Gesichter in den Bildern erkannt und ihre Merkmale analysiert werden.\r\n",
        "\r\n",
        "![Ein Roboter, der ein Gesicht analysiert](./images/face_analysis.jpg)\r\n",
        "\r\n",
        "## Verwenden des Cognitive Service „Gesichtserkennung“, um Gesichter zu erkennen\r\n",
        "\r\n",
        "Angenommen, das Smart-Store-System für Northwind Traders soll Kunden erkennen und ihre Gesichtsmerkmale analysieren können. In Microsoft Azure können Sie zu diesem Zweck die **Gesichtserkennung** verwenden, die zu den Azure Cognitive Services gehört.\r\n",
        "\r\n",
        "### Erstellen einer Cognitive Services-Ressource\r\n",
        "\r\n",
        "Erstellen Sie zunächst eine **Cognitive Services**-Ressource in Ihrem Azure-Abonnement.\r\n",
        "\r\n",
        "> **Hinweis**: Falls Sie bereits eine Cognitive Services-Ressource haben, können Sie die entsprechende **Schnellstart**-Seite im Azure-Portal öffnen und den Schlüssel und den Endpunkt der Ressource unten in die Zelle kopieren. Führen Sie andernfalls die folgenden Schritte aus, um eine Ressource zu erstellen.\r\n",
        "\r\n",
        "1. Öffnen Sie das Azure-Portal unter „https://portal.azure.com“ in einer neuen Browserregisterkarte, und melden Sie sich mit Ihrem Microsoft-Konto an.\r\n",
        "2. Klicken Sie auf die Schaltfläche **&#65291;Ressource erstellen**, suchen Sie nach *Cognitive Services*, und erstellen Sie eine **Cognitive Services**-Ressource mit den folgenden Einstellungen:\r\n",
        "    * **Abonnement**: *Ihr Azure-Abonnement*\r\n",
        "    * **Ressourcengruppe**: *Wählen Sie eine Ressourcengruppe aus, oder erstellen Sie eine Ressourcengruppe mit einem eindeutigen Namen.*\r\n",
        "    * **Region**: *Wählen Sie eine verfügbare Region aus*:\r\n",
        "    * **Name**: *Geben Sie einen eindeutigen Namen ein.*\r\n",
        "    * **Tarif**: S0\r\n",
        "    * **Ich bestätige, dass ich die Hinweise gelesen und verstanden habe**: Ausgewählt\r\n",
        "3. Warten Sie, bis die Bereitstellung abgeschlossen ist. Öffnen Sie anschließend Ihre Cognitive Services-Ressource, und klicken Sie auf der Seite **Übersicht** auf den Link zur Schlüsselverwaltung für den Dienst. Sie benötigen den Endpunkt und Schlüssel, um sich aus Clientanwendungen heraus mit Ihrer Cognitive Services-Ressource zu verbinden.\r\n",
        "\r\n",
        "### Abrufen des Schlüssels und Endpunkts für Ihre Cognitive Services-Ressource\r\n",
        "\r\n",
        "Um Ihre Cognitive Services-Ressource verwenden zu können, benötigen Clientanwendungen deren Endpunkt und Authentifizierungsschlüssel:\r\n",
        "\r\n",
        "1. Kopieren Sie im Azure-Portal auf der Seite **Schlüssel und Endpunkt** für Ihre Cognitive Service-Ressource den **Schlüssel1** für Ihre Ressource, und fügen Sie ihn im unten stehenden Code anstelle von **YOUR_COG_KEY** ein.\r\n",
        "\r\n",
        "2. Kopieren Sie den **Endpunkt** für Ihre Ressource, und fügen Sie ihn unten im Code anstelle von **YOUR_COG_ENDPOINT** ein.\r\n",
        "\r\n",
        "3. Führen Sie die folgende Codezelle aus, indem Sie oben links in der Zelle auf die Schaltfläche „Zelle ausführen“ <span>&#9655;</span> klicken."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "cog_key = 'YOUR_COG_KEY'\n",
        "cog_endpoint = 'YOUR_COG_ENDPOINT'\n",
        "\n",
        "print('Ready to use cognitive services at {} using key {}'.format(cog_endpoint, cog_key))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599693964655
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jetzt haben Sie eine Cognitive Services-Ressource, mit der Sie den Gesichtserkennungsdienst verwenden können, um menschliche Gesichter im Geschäft zu erkennen.\r\n",
        "\r\n",
        "Führen Sie die unten stehende Codezelle aus, um ein Beispiel zu sehen."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.cognitiveservices.vision.face import FaceClient\n",
        "from msrest.authentication import CognitiveServicesCredentials\n",
        "from python_code import faces\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "# Create a face detection client.\r\n",
        "face_client = FaceClient(cog_endpoint, CognitiveServicesCredentials(cog_key))\n",
        "\n",
        "# Open an image\r\n",
        "image_path = os.path.join('data', 'face', 'store_cam2.jpg')\n",
        "image_stream = open(image_path, \"rb\")\n",
        "\n",
        "# Detect faces\r\n",
        "detected_faces = face_client.face.detect_with_stream(image=image_stream)\n",
        "\n",
        "# Display the faces (code in python_code/faces.py)\r\n",
        "faces.show_faces(image_path, detected_faces)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599693970079
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jedem erkannten Gesicht wird eine eindeutige ID zugeordnet, damit Ihre Anwendung die einzelnen erkannten Gesichter identifizieren kann.\r\n",
        "\r\n",
        "Führen Sie die folgende Zelle aus, um die IDs für weitere Gesichter von Käufern anzuzeigen."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Open an image\r\n",
        "image_path = os.path.join('data', 'face', 'store_cam3.jpg')\n",
        "image_stream = open(image_path, \"rb\")\n",
        "\n",
        "# Detect faces\r\n",
        "detected_faces = face_client.face.detect_with_stream(image=image_stream)\n",
        "\n",
        "# Display the faces (code in python_code/faces.py)\r\n",
        "faces.show_faces(image_path, detected_faces, show_id=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599693970447
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysieren von Gesichtsmerkmalen\r\n",
        "\r\n",
        "Mit der Gesichtserkennung können Sie aber nicht nur Gesichter erkennen. Sie können auch Gesichtsmerkmale und Gesichtsausdrücke analysieren, um Alter und Gefühlszustand zu erkennen. Führen Sie den folgenden Code aus, um die Gesichtsmerkmale eines Käufers zu analysieren."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Open an image\r\n",
        "image_path = os.path.join('data', 'face', 'store_cam1.jpg')\n",
        "image_stream = open(image_path, \"rb\")\n",
        "\n",
        "# Detect faces and specified facial attributes\r\n",
        "attributes = ['age', 'emotion']\n",
        "detected_faces = face_client.face.detect_with_stream(image=image_stream, return_face_attributes=attributes)\n",
        "\n",
        "# Display the faces and attributes (code in python_code/faces.py)\r\n",
        "faces.show_face_attributes(image_path, detected_faces)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599693971321
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gemäß der Emotionsbewertung des im Bild gezeigten Kunden scheint dieser mit seinem Einkaufserlebnis recht zufrieden zu sein.\r\n",
        "\r\n",
        "## Suchen ähnlicher Gesichter \r\n",
        "\r\n",
        "Mit den Gesichts-IDs für die erkannten Bilder können Sie erkannte Gesichter einzeln identifizieren. Sie können diese IDs verwenden, um ein erkanntes Gesicht mit zuvor erkannten Gesichtern zu vergleichen und Gesichter mit ähnlichen Merkmalen zu suchen.\r\n",
        "\r\n",
        "Führen Sie zum Beispiel die folgende Zelle aus, um den Käufer in einem Bild mit Kunden in einem anderen Bild zu vergleichen und ein ähnliches Gesicht zu finden."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the ID of the first face in image 1\r\n",
        "image_1_path = os.path.join('data', 'face', 'store_cam3.jpg')\n",
        "image_1_stream = open(image_1_path, \"rb\")\n",
        "image_1_faces = face_client.face.detect_with_stream(image=image_1_stream)\n",
        "face_1 = image_1_faces[0]\n",
        "\n",
        "# Get the face IDs in a second image\r\n",
        "image_2_path = os.path.join('data', 'face', 'store_cam2.jpg')\n",
        "image_2_stream = open(image_2_path, \"rb\")\n",
        "image_2_faces = face_client.face.detect_with_stream(image=image_2_stream)\n",
        "image_2_face_ids = list(map(lambda face: face.face_id, image_2_faces))\n",
        "\n",
        "# Find faces in image 2 that are similar to the one in image 1\r\n",
        "similar_faces = face_client.face.find_similar(face_id=face_1.face_id, face_ids=image_2_face_ids)\n",
        "\n",
        "# Show the face in image 1, and similar faces in image 2(code in python_code/face.py)\r\n",
        "faces.show_similar_faces(image_1_path, face_1, image_2_path, image_2_faces, similar_faces)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599693972555
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Erkennen von Gesichtern\r\n",
        "\r\n",
        "Bisher haben wir Ihnen gezeigt, wie Sie mit der Gesichtserkennung Gesichter und deren Merkmale erkennen sowie Gesichter suchen können, die einander ähneln. Als nächsten Schritt können Sie eine *Gesichtserkennungslösung* implementieren, in der Sie die Gesichtserkennung dahingehend trainieren, das Gesicht einer bestimmten Person zu erkennen. Dies ist für verschiedene Szenarien hilfreich, etwa um Fotos von Freunden in einer Social-Media-Anwendung automatisch zu markieren oder um die Gesichtserkennung für eine biometrische Identitätsüberprüfung zu nutzen.\r\n",
        "\r\n",
        "Gehen wir hierzu davon aus, dass Northwind Traders mit einem Gesichtserkennungssystem sicherstellen möchte, dass nur autorisierte Mitarbeiter aus der IT-Abteilung auf gesicherte Systeme zugreifen können.\r\n",
        "\r\n",
        "Zunächst erstellen wir eine *Personengruppe* für die autorisierten Mitarbeiter."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "group_id = 'employee_group_id'\n",
        "try:\n",
        "    # Delete group if it already exists\n",
        "    face_client.person_group.delete(group_id)\n",
        "except Exception as ex:\n",
        "    print(ex.message)\n",
        "finally:\n",
        "    face_client.person_group.create(group_id, 'employees')\n",
        "    print ('Group created!')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599693973492
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nachdem wir die *Personengruppe* erstellt haben, können wir für jeden Mitarbeiter, der zur Gruppe gehören soll, eine *Person* hinzufügen und mehrere Fotos für jede Person registrieren, damit die Gesichtserkennung lernen kann, die Gesichtsmerkmale der einzelnen Personen zu erkennen. Idealerweise sollten wir dazu jeweils Bilder der Personen in unterschiedlichen Haltungen und mit unterschiedlichen Gesichtsausdrücken verwenden.\r\n",
        "\r\n",
        "Wir fügen einen Mitarbeiter namens „Wendell“ hinzu und registrieren drei Fotos von ihm."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import os\n",
        "%matplotlib inline\n",
        "\n",
        "# Add a person (Wendell) to the group\r\n",
        "wendell = face_client.person_group_person.create(group_id, 'Wendell')\n",
        "\n",
        "# Get photo's of Wendell\r\n",
        "folder = os.path.join('data', 'face', 'wendell')\n",
        "wendell_pics = os.listdir(folder)\n",
        "\n",
        "# Register the photos\r\n",
        "i = 0\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "for pic in wendell_pics:\n",
        "    # Add each photo to person in person group\n",
        "    img_path = os.path.join(folder, pic)\n",
        "    img_stream = open(img_path, \"rb\")\n",
        "    face_client.person_group_person.add_face_from_stream(group_id, wendell.person_id, img_stream)\n",
        "\n",
        "    # Display each image\n",
        "    img = Image.open(img_path)\n",
        "    i +=1\n",
        "    a=fig.add_subplot(1,len(wendell_pics), i)\n",
        "    a.axis('off')\n",
        "    imgplot = plt.imshow(img)\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599693976898
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nachdem wir die Person hinzugefügt und die Fotos registriert haben, können wir die Gesichtserkennung für die jeweilige Person trainieren."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "face_client.person_group.train(group_id)\n",
        "print('Trained!')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599693977046
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nachdem Sie das Modell trainiert haben, können Sie es verwenden, um die erkannten Gesichter in einem Bild zu identifizieren."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the face IDs in a second image\r\n",
        "image_path = os.path.join('data', 'face', 'employees.jpg')\n",
        "image_stream = open(image_path, \"rb\")\n",
        "image_faces = face_client.face.detect_with_stream(image=image_stream)\n",
        "image_face_ids = list(map(lambda face: face.face_id, image_faces))\n",
        "\n",
        "# Get recognized face names\r\n",
        "face_names = {}\n",
        "recognized_faces = face_client.face.identify(image_face_ids, group_id)\n",
        "for face in recognized_faces:\n",
        "    person_name = face_client.person_group_person.get(group_id, face.candidates[0].person_id).name\n",
        "    face_names[face.face_id] = person_name\n",
        "\n",
        "# show recognized faces\r\n",
        "faces.show_recognized_faces(image_path, image_faces, face_names)\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1599693994820
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Weitere Informationen\r\n",
        "\r\n",
        "Weitere Informationen zum Cognitive Service „Gesichtserkennung“ finden Sie in der [Dokumentation für die Gesichtserkennung](https://docs.microsoft.com/azure/cognitive-services/face/).\r\n"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}